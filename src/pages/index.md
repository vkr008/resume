**"Currently in the notice period, with early option for joining."**

**Vikas Kumar**

![Icon

Description automatically generated](Aspose.Words.cd1d85e7-b0d5-4d61-b87c-4f985511b106.001.png)contact@vikaskumar.in

![Icon

Description automatically generated](Aspose.Words.cd1d85e7-b0d5-4d61-b87c-4f985511b106.002.png)Bangalore, Karnataka 560100      [+91-89715-76342](tel:+918971576342)


**Summary**

*(In brief - Data Architect: Skilled in engineering and constructing cost-effective, high-quality optimized pipelines!)*

*Total Exp. 10.6 yrs*

***Since the beginning of my career, I have consistently exhibited a strong passion for data and data-driven products and tools. I take great pleasure in connecting the dots to create meaningful business insights and decision-making processes. My enjoyment stems from visualizing the transformation of raw data into valuable pieces of information. I have been fortunate to work with cutting-edge tools that have enabled me to excel in this field. The recent surge in Big Data has further intensified my enthusiasm, making it an even more exciting time to be involved in this field. "Show me the data!"***



- Proficient in designing, creating, analyzing, and optimizing high-quality data-driven products using a variety of tools designed for handling Big Data. Additionally, experienced in working with legacy tools and adapting to new technology.
- Experienced in designing application and architecture for real-time decision-making processes, customized for various industries.
- Developed a keen interest in understanding business needs and pain points, which is crucial in delivering data-driven decisions that cater to business requirements.
- Strongly committed to continuous improvement and delivering projects within the stipulated timelines.
- Highly enthusiastic to explore new technology such as AWS APIs, Lambda, AWS EC2, S3, RDS, Snowflake or enterprise-based big data tools like Cloudera Hive, HBase, Druid, Elastic Search etc. In addition, continually researching and adopting new ETL and reporting tools like Informatica, Matillion, PowerBI, Tableau, and Business Objects Analysis.
- Experienced in writing SQL queries and discovering patterns to automate tedious tasks, coding in scripting tools like Unix and Python.
- Experienced in designing data models that are efficient and can map the real world, finding solutions for business problems that offer immense satisfaction - a perfect blend of art and technology!
- Proactive in learning new technologies during free time, such as serverless cloud offerings, machine learning, and data visualization, to stay ahead of the curve.

**Technical Expertise**												

Programming Languages: 	Python, SQL, Unix Shell Scripting, PLSQL, Hive.

Data Analysis Tools: 		PowerBI, Tableau, R Programming, Excel, SAP Business Objects.

Cloud Platforms: 			AWS - Lamdba, API, EC2, RDS, S3, CloudWatch. Azure – DevOPS.

NoSQL Databases: 		HBase, Druid, ElasticSearch.

Relational Databases:		 Snowflake, PostgreSQL, Oracle, Exadata, MySQL, SyBase IQ.

Scheduling Tools: 		Control M Desktop, Crontab (Unix).

ETL Tools: 			Matillion, Informatica PowerCenter (Designer, Workflow, Monitor).

Operating Systems: 		MacOS, Unix, Linux, Windows.

Code Editors: 			PyCharm, BBEdit, TextMate, Notepad++, Beyond Compare.

Version Control Systems: 	Git (for Databases as well), SVN.

Industry Experience: 		Digital Marketing, Manufacturing, Investment Banking, Digital Banking.


**Experience**														

![Logo for iQuanti, Inc.](Aspose.Words.cd1d85e7-b0d5-4d61-b87c-4f985511b106.003.png)

` `**IQuanti – Staff Engineer; Bangalore, Karnataka, India (Apr 20’ to -) : Serving Notice.**

`	`**(Product Team - ALPS)**

1. Played a pivotal role in the inception and design of the company's Data Lake and Data Warehouse architecture, collaborating with cross-functional teams from the beginning to define the data requirements and business objectives.
1. Designed and implemented a scalable and cost-effective Data Lake on AWS S3, ensuring seamless data ingestion, storage, and processing using AWS Glue for data cataloging and Lambda functions for serverless data processing.
1. Led the selection and evaluation process of various data tools and technologies, recommending Snowflake as the ideal choice for our cloud-native Data Warehouse, based on its scalability, performance, and cost-effectiveness.
1. Spearheaded the development and deployment of the Data Warehouse on Snowflake, taking charge of setting up security role policies to ensure data integrity and access control across the platform.
1. Actively participated in architecting an Enterprise Data Platform (EDP) leveraging Snowflake pipes and secure data sharing capabilities, allowing teams to collaborate effectively and share critical data insights securely.
1. Advocated and implemented Row Level Security (RLS) within the Data Warehouse, ensuring data access is restricted based on user roles and permissions, safeguarding sensitive information from unauthorized access.
1. Led and mentored a high-performing team of data professionals and BI designers, fostering a collaborative and innovative culture, resulting in successful project deliveries and skill development.
1. Continuously drove innovation in Datalake and ETL processes, exploring and adopting cutting-edge technologies and methodologies to enhance the organization's data-driven capabilities and strategic decision-making.
1. Successfully introduced CI/CD processes using Git for database components, streamlining the deployment of database objects such as procedures and tables, leading to increased agility and reduced development cycles using python module schemachange.
1. Implemented a robust alerting and governance pipeline, integrating data quality checks and automated alerts to ensure data accuracy and reliability at every stage of the data pipeline.
1. Proactively sought opportunities to optimize costs and improve overall efficiency, leveraging cloud technology's benefits, which resulted in significant cost savings and improved data processing times.
1. Collaborated with stakeholders from different teams to define data requirements for embedded reports and dashboards within the product, enhancing the user experience and providing valuable insights to customers.

As a team leader in the strategic division of Digital Marketing, I am accountable for developing cutting-edge products that surpass customer expectations. This involves designing high-performing data-driven solutions, carefully selecting the most suitable tools to support the massive amounts of data underpinning the platform, and delivering responsive front-end interfaces that provide lightning-fast access to information. Additionally, I am committed to ensuring that our products are released to market with exceptional speed, implementing continuous improvement practices and optimizing code where possible.

As a team member, I also oversee the data quality and governance of the entire system, from data staging to final reporting. By establishing and enforcing strict standards, I ensure that the complex metrics and models that underpin our products function as intended. This is critical given the significant volumes of ever-increasing data that support our products. I am meticulous in my analysis of data, processing vast quantities from multiple cloud-based platforms, including Amazon EC2, RDS, S3, Snowflake, and ultimately delivering reports that meet or exceed client expectations.

![Logo for Capgemini](Aspose.Words.cd1d85e7-b0d5-4d61-b87c-4f985511b106.004.jpeg)

` `**Capgemini – Data Engineer; Bangalore, Karnataka, India (Apr 19’ to Mar 20’)**

`	`**(For a major US Bank)**

My responsibilities include creating a new data pipeline using various big data tools from scratch. Additionally, I am involved in writing a wrapper around an existing data ingestion framework. As part of my role, I work on data modeling for storing data in SyBase IQ from upstream systems like Finacle. I am also leading a new initiative to develop a user interface that enables users to input attributes for input, output, and transformation processes.

![Logo for Introlligent Inc.](Aspose.Words.cd1d85e7-b0d5-4d61-b87c-4f985511b106.005.jpeg)

`  `**Introlligent Inc.  – Data Quality Analyst; Bangalore, Karnataka, India (Sep 16’ to Apr 19’)**

`	`**Manufacturing Quality Management (For Major Electronic Manufacturing Major)** 

**Worked on data coming from manufacturing sites from across different vendors.**

Over the past 2.9 years, I had the privilege to spearhead the development and implementation of a cutting-edge system leveraging state-of-the-art technologies such as Hadoop, Hive, MySQL, Python, HBase, ElasticSearch, and more. As the leader of a dedicated team, we ensured that the data flowing into the system was of high quality and aligned with business objectives, while also working closely with users to facilitate a smooth transition and exceed their expectations. It's truly gratifying to receive a lot of positive feedback from users.

My role also entailed enforcing strict data quality standards across various products and factories, by validating around 40 key metrics and applying rigorous data exploration techniques. I collaborated with multiple stakeholders, including vendors from China, to address and resolve any data issues that arose, often writing ad hoc scripts and creating automated batch jobs to streamline operations.

- Ensure timely delivery of multiple reports daily to enable smooth production in the factory.
- Act quickly to resolve time-critical issues that could cause downtime in the production line.
- Coordinate with vendors to obtain critical data required for smooth operations.
- Develop and implement email alerts and reports using tools like Tableau for exploratory analysis, shared on servers for stakeholders.
- Establish an in-house data ingestion system for multiple sources, contributing to the creation of a single point of truth.
- Regularly collaborate with FAs in the factory and the New Product team.
- Work with various stakeholders such as NPI, Dev, QA, R&D, Support, and Reporting teams to accomplish various tasks.
- Assist in the migration from flat file to JSON structure from the source.

![Logo for Tata consultancy services](Aspose.Words.cd1d85e7-b0d5-4d61-b87c-4f985511b106.006.jpeg)

`  `[**Tata Consultancy Services Ltd. (TCS) – System Engineer; Bangalore, Karnataka, India**](http://www.tcs.com/Pages/default.aspx)

`	`**CRO IT (For a major Swiss Bank) (Mar 13’ to Sep 16’)**

**Working for CoE (Center of Excellence), CCAR (Comprehensive Capital Analysis and Review), Data Lineage Team, BFI etc.**

As part of my responsibilities, I have been developing proof-of-concept models and evaluating major big data stacks such as Spark and Hadoop. Using Python and Shell scripting, I have automated existing processes to improve efficiency.

One of the major systems that I have been working on involves ETL operations on RDS, which is used to calculate risk numbers such as VAR. Data is received in the form of feeds from different geographies and in different formats from various upstream RMS. To ensure data quality, I perform ETL operations, including cleansing, profiling, applying business rules, and performing calculations.

Once the data is processed, reports are generated on MARS and STAR systems. This involves working closely with different stakeholders across the organization to ensure data accuracy and report functionality.

- Integrated data from various sources globally, including flat files, web services, excel sheets, and CSVs.
- Designed and implemented mappings for new regulatory projects, ensuring complete compliance with requirements.
- Collaborated with business stakeholders to develop technical documentation and lead the implementation of code changes within JIRA's specified SLAs, from unit testing to production release.
- Resolved critical production and UAT incidents for the system, requiring advanced SQL queries.
- Worked across different Risk Management Systems (RMS) with varying mapping codes to load different feeds.
- Developed various PL/SQL procedures to optimize tasks and created mapping codes based on specific requirements.
- Managed a team responsible for creating new feeds for the regulatory project CCAR.
- Conducted complex ad hoc analysis by writing SQL queries.
- Performed volumetric analysis and tested all upstream and downstream systems' impacts for new code implementations.
- Created a Proof of Concept (POC) for regulatory requirements by developing Data Lineage and participated in client interactions to obtain sign-off for final production labels.
- Served as a subject matter expert (SME) for two complex systems: SCI and Scenario feeds.
- Collaborated with third-party auditors to ensure compliance with regulatory requirements.
- Automated task scheduling in Control M using UNIX and SQL scripts.
- Developed and tested jobs in Control M and analyzed over 10k Control M jobs to improve efficiency.
- Created Python scripts to automate daily tasks and generate HTML using XSLT transformation for XML data.

**ILP Trainer –Trivandrum (TCS Internal) - Learning Enabler**

In this role, I was responsible for facilitating the onboarding of new hires and ensuring their seamless integration into the data warehousing team. I provided training to new employees to help them gain a thorough understanding of the technical aspects of the job, starting from creating a new website based on the business model up to generating business-critical reports and identifying key performance indicators (KPIs). My aim was to enable them to effectively visualize the data and use it to make informed business decisions.

- Conducted research on data quality issues related to in-house ETL/BI case studies and contributed to their resolution.
- Created new case studies and developed data models with different dimensions for internal use in training programs.
- Designed and developed new data models based on STAR and SNOWFLAKE schemes in ORACLE database for all new case studies.
- Built new static tables like Data Dimensions for reporting purposes using Business Objects.
- Developed new data models for different domains to impart training on various concepts used in BIPM process.
- Created business-critical reports using Business Objects, designed universe, and built reports on top of it.




**Education**							 							

|<p>![Logo for Visvesvaraya Technological University](Aspose.Words.cd1d85e7-b0d5-4d61-b87c-4f985511b106.007.png)</p><p>`         `[**Visvesvaraya Technological University**](http://vtu.ac.in/)</p>|<p></p><p>`         `Bachelor of Engineering</p><p></p><p></p>|<p></p><p>2012</p>|
| :- | :- | -: |

**Activities** 				                                                    							

- Collaborated on the aesthetic enhancement of TCS offices by painting walls and galleries.
- Participated in CSR initiatives aimed at mentoring and teaching underprivileged students.
- Utilized artistic skills to create caricature designs.
- Organized team events to promote team building and foster a positive work environment.
- Coordinated a large-scale event, the 'IT Wiz Quiz,' which attracted over 2000 student participants.

**On-Site Exposure**				                                                    						

- Traveled to various cities in China for work-related purposes on multiple occasions.

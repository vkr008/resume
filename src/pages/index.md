+-----------------------+-----------------------+-----------------------+
| > **[\"Currently in   | ![](verto             | *linkedin.            |
| > the notice period,  | pal_1dc4c6c3f4bc43f4b | com/in/**vikasraut*** |
| > with early option   | a36e2adf171d4cb/media |                       |
| > for                 | /image1.png){width="0 |                       |
| > join                | .13194444444444445in" |                       |
| ing.\"]{.underline}** | height="0.            |                       |
|                       | 13194444444444445in"} |                       |
+=======================+=======================+=======================+
| **VR**                |                       |                       |
|                       |                       |                       |
| > ![](verto           |                       |                       |
| pal_1dc4c6c3f4bc43f4b |                       |                       |
| a36e2adf171d4cb/media |                       |                       |
| /image2.png){width="0 |                       |                       |
| .15694444444444444in" |                       |                       |
| > height="0.1         |                       |                       |
| 1249890638670167in"}\ |                       |                       |
| > Bangalore,          |                       |                       |
| > Kar[7               |                       |                       |
| 15-76342]{.underline} |                       |                       |
+-----------------------+-----------------------+-----------------------+

**[Summary]{.underline}**\
*(In brief - Data Evangelist: Skilled in engineering and constructing
cost-effective, high-quality optimized pipelines!)*

*Total Exp. 10.6 yrs*

***Since the beginning of my career, I have consistently exhibited a
strong passion for data and data-driven products and tools. I take great
pleasure in connecting the dots to create meaningful business insights
and decision-making processes. My enjoyment stems from visualizing the
transformation of raw data into valuable pieces of information. I have
been fortunate to work with cutting-edge tools that have enabled me to
excel in this field. The recent surge in Big Data has further
intensified my enthusiasm, making it an even more exciting time to be
involved in this field. \"Show me the data!\"***

> •Proficient in creating, analyzing, and optimizing high-quality
> data-driven products using Big Data tools, with a proven track record
> of adapting to new technologies and legacy systems.
>
> •Experienced in designing real-time decision-making architectures
> tailored to diverse industries, while understanding business needs and
> pain points to deliver data-driven decisions aligned with
> requirements. •Strongly committed to continuous improvement,
> consistently delivering projects on time, and leveraging a diverse
> tech stack for insightful data-driven decisions.
>
> •Successfully executed Proof of Concepts (POCs) and productized
> systems, utilizing Druid for high- performance time-series data
> analysis across multiple dimensions\
> •Integrated cutting-edge tools like AWS APIs, Lambda, EC2, S3, RDS,
> and big data tools such as Cloudera Hive, HBase, and Elastic Search,
> while continuously exploring and adopting ETL and reporting tools like
> Informatica, Matillion, PowerBI, Tableau, and Business Objects
> Analysis.
>
> •Proficient in writing SQL queries, automating tasks with Unix and
> Python scripting, and discovering valuable patterns to optimize
> processes and enhance efficiency.
>
> •Skilled in designing efficient data models that map real-world
> scenarios and deliver solutions offering immense satisfaction,
> combining the art and technology of data engineering.
>
> •Passionate about continuous learning and staying ahead of the curve
> by exploring serverless cloud offerings, machine learning, data
> visualization, and other emerging technologies.
>
> **Technical Expertise**

+-----------------------------------+-----------------------------------+
| > Programming Languages: Data     | > Python, SQL, Unix Shell         |
| > Analysis Tools:\                | > Scripting, PLSQL, Hive.         |
| > Cloud Platforms:\               | >                                 |
| > NoSQL Databases:\               | > PowerBI, Tableau, R             |
| > Relational Databases:\          | > Programming, Excel, SAP         |
| > Scheduling Tools:\              | > Business Objects.               |
| > ETL Tools:\                     | >                                 |
| > Operating Systems:\             | > AWS - Lamdba, API, EC2, RDS,    |
| > Code Editors:\                  | > S3, CloudWatch. Azure --        |
| > Version Control Systems:        | > DevOPS. Druid, HBase,           |
| > Industry Experience:            | > ElasticSearch.                  |
|                                   | >                                 |
|                                   | > Snowflake, PostgreSQL, Oracle,  |
|                                   | > Exadata, MySQL, SyBase IQ.      |
|                                   | >                                 |
|                                   | > Control M Desktop, Crontab      |
|                                   | > (Unix).                         |
|                                   | >                                 |
|                                   | > Matillion, Informatica          |
|                                   | > PowerCenter (Designer,          |
|                                   | > Workflow, Monitor). MacOS,      |
|                                   | > Unix, Linux, Windows.           |
|                                   | >                                 |
|                                   | > PyCharm, BBEdit, TextMate,      |
|                                   | > Notepad++, Beyond Compare.      |
|                                   | >                                 |
|                                   | > Git (for Databases as well),    |
|                                   | > SVN.                            |
|                                   | >                                 |
|                                   | > Digital Marketing,              |
|                                   | > Manufacturing, Investment       |
|                                   | > Banking, Digital Banking.       |
+===================================+===================================+
+-----------------------------------+-----------------------------------+

**Experience**

> ![](vertopal_1dc4c6c3f4bc43f4ba36e2adf171d4cb/media/image3.png){width="0.375in"
> height="0.375in"} **IQuanti -- Staff Engineer; Bangalore, Karnataka,
> India (Apr 20' to -): Serving Notice.**
>
> **(Product Team - ALPS)**
>
> 1.Spearheading the design and implementation of a cutting-edge Data
> pipeline leveraging APIs and AWS services, including S3, Glue, Lambda,
> Snowflake, and Matillion. This pipeline will provide valuable insights
> and data visualizations via embedded reports and dashboards within our
> product. Proficient on setting up security role policies etc using
> various tools.
>
> 2.Proficient in setting up security role policies and implementing an
> Enterprise Data Platform (EDP) with Snowflake pipes, secure data
> sharing, real-time streaming, and Row Level Security (RLS).
>
> 3.Leading and mentoring a team of highly skilled data professionals
> and BI designers to deliver innovative solutions and drive growth.
>
> 4.Continuously innovating new solutions in Datalake and ETL to enhance
> our data-driven capabilities and support strategic decision-making.
>
> 5.Facilitating the implementation of new CI/CD processes using Git for
> seamless deployment of DB components such as procedures and tables.
>
> 6.Establishing a robust alerting and governance pipeline to ensure the
> quality and accuracy of data at every stage of the pipeline.
>
> 7.Proactively exploring new opportunities to optimize costs, reduce
> execution time, and improve overall efficiency, with a particular
> focus on maximizing the benefits of cloud technology.

As a team leader in the strategic division of Digital Marketing, I am
accountable for developing cutting-edge products that surpass customer
expectations. This involves designing high-performing data-driven
solutions, carefully selecting the most suitable tools to support the
massive amounts of data underpinning the platform, and delivering
responsive front-end interfaces that provide lightning-fast access to
information. Additionally, I am committed to ensuring that our products
are released to market with exceptional speed, implementing continuous
improvement practices and optimizing code where possible.

As a team member, I also oversee the data quality and governance of the
entire system, from data staging to final reporting. By establishing and
enforcing strict standards, I ensure that the complex metrics and models
that underpin our products function as intended. This is critical given
the significant volumes of ever-increasing data that support our
products. I am meticulous in my analysis of data, processing vast
quantities from multiple cloud-based platforms, including Amazon EC2,
RDS, S3, Snowflake, and ultimately delivering reports that meet or
exceed client expectations.

![](vertopal_1dc4c6c3f4bc43f4ba36e2adf171d4cb/media/image4.png){width="0.3958333333333333in"
height="0.3958333333333333in"} **Capgemini -- Data Engineer; Bangalore,
Karnataka, India (Apr 19' to Mar 20')**

> **(For a major US Bank)**

My responsibilities include creating a new data pipeline using various
big data tools from scratch. Additionally, I am involved in writing a
wrapper around an existing data ingestion framework. As part of my role,
I work on data modeling for storing data in SyBase IQ from upstream
systems like Finacle. I am also leading a new initiative to develop a
user interface that enables users to input attributes for input, output,
and transformation processes.

![](vertopal_1dc4c6c3f4bc43f4ba36e2adf171d4cb/media/image5.png){width="0.5208333333333334in"
height="0.5208333333333334in"} **Introlligent Inc. -- Data Quality
Analyst; Bangalore, Karnataka, India (Sep 16' to Apr 19')**

> **Manufacturing Quality Management (For Major Electronic Manufacturing
> Major)**

**Worked on data coming from manufacturing sites from across different
vendors.**

Over the past 2.9 years, I led a groundbreaking real-time Druid POC and
governance quality report system in a factory production environment.
Leveraging cutting-edge technologies like Hadoop, Hive, MySQL, Python,
HBase, Elasticsearch, and more, our system ensured high-quality data
aligned with business objectives. Positive feedback from users
highlighted its transformative impact, detecting and addressing wrong
practices in real-time for improved efficiency and compliance.

My role also entailed enforcing strict data quality standards across
various products and factories, by validating around 40 key metrics and
applying rigorous data exploration techniques. I collaborated with
multiple stakeholders, including vendors from China, to address and
resolve any data issues that arose, often writing ad hoc scripts and
creating automated batch jobs to streamline operations.

> •Ensure timely delivery of multiple reports daily to enable smooth
> production in the factory. •Act quickly to resolve time-critical
> issues that could cause downtime in the production line.
>
> •Coordinate with vendors to obtain critical data required for smooth
> operations.
>
> •Develop and implement email alerts and reports using tools like
> Tableau for exploratory analysis, shared on servers for stakeholders.
>
> •Establish an in-house data ingestion system for multiple sources,
> contributing to the creation of a single point of truth.
>
> •Regularly collaborate with FAs in the factory and the New Product
> team.
>
> •Work with various stakeholders such as NPI, Dev, QA, R&D, Support,
> and Reporting teams to accomplish various tasks.
>
> •Assist in the migration from flat file to JSON structure from the
> source.
>
> ![](vertopal_1dc4c6c3f4bc43f4ba36e2adf171d4cb/media/image6.png){width="0.4152777777777778in"
> height="0.4097222222222222in"}
>
> **CRO IT (For a major Swiss Bank) (Mar 13' to Sep 16')**

**Working for CoE (Center of Excellence), CCAR (Comprehensive Capital
Analysis and Review), Data Lineage Team, BFI etc.**

As part of my responsibilities, I have been developing proof-of-concept
models and evaluating major big data stacks such as Spark and Hadoop.
Using Python and Shell scripting, I have automated existing processes to
improve efficiency.

One of the major systems that I have been working on involves ETL
operations on RDS, which is used to calculate risk numbers such as VAR.
Data is received in the form of feeds from different geographies and in
different formats from various upstream RMS. To ensure data quality, I
perform ETL operations, including cleansing, profiling, applying
business rules, and performing calculations.

Once the data is processed, reports are generated on MARS and STAR
systems. This involves working closely with different stakeholders
across the organization to ensure data accuracy and report
functionality.

> •Integrated data from various sources globally, including flat files,
> web services, excel sheets, and CSVs.
>
> •Designed and implemented mappings for new regulatory projects,
> ensuring complete compliance with requirements.
>
> •Collaborated with business stakeholders to develop technical
> documentation and lead the implementation of code changes within
> JIRA\'s specified SLAs, from unit testing to production release.
>
> •Resolved critical production and UAT incidents for the system,
> requiring advanced SQL queries.
>
> •Worked across different Risk Management Systems (RMS) with varying
> mapping codes to load different feeds.
>
> •Developed various PL/SQL procedures to optimize tasks and created
> mapping codes based on specific requirements.
>
> •Managed a team responsible for creating new feeds for the regulatory
> project CCAR.
>
> •Conducted complex ad hoc analysis by writing SQL queries.
>
> •Performed volumetric analysis and tested all upstream and downstream
> systems\' impacts for new code implementations.
>
> •Created a Proof of Concept (POC) for regulatory requirements by
> developing Data Lineage and participated in client interactions to
> obtain sign-off for final production labels.
>
> •Served as a subject matter expert (SME) for two complex systems: SCI
> and Scenario feeds. •Collaborated with third-party auditors to ensure
> compliance with regulatory requirements.
>
> •Automated task scheduling in Control M using UNIX and SQL scripts.
>
> •Developed and tested jobs in Control M and analyzed over 10k Control
> M jobs to improve efficiency. •Created Python scripts to automate
> daily tasks and generate HTML using XSLT transformation for XML data.
>
> **ILP Trainer --Trivandrum (TCS Internal) - Learning Enabler**

In this role, I was responsible for facilitating the onboarding of new
hires and ensuring their seamless integration into the data warehousing
team. I provided training to new employees to help them gain a thorough
understanding of the technical aspects of the job, starting from
creating a new website based on the business model up to generating
business-critical reports and identifying key performance indicators
(KPIs). My aim was to enable them to effectively visualize the data and
use it to make informed business decisions.

> •Conducted research on data quality issues related to in-house ETL/BI
> case studies and contributed to their resolution.
>
> •Created new case studies and developed data models with different
> dimensions for internal use in training programs.
>
> •Designed and developed new data models based on STAR and SNOWFLAKE
> schemes in ORACLE database for all new case studies.
>
> •Built new static tables like Data Dimensions for reporting purposes
> using Business Objects. •Developed new data models for different
> domains to impart training on various concepts used in BIPM process.
>
> • Created business-critical reports using Business Objects, designed
> universe, and built reports on top of it.

**Education**

+-----------------------+-----------------------+-----------------------+
| > ![](verto           | > Bachelor of         | 2012                  |
| pal_1dc4c6c3f4bc43f4b | > Engineering         |                       |
| a36e2adf171d4cb/media |                       |                       |
| /image7.png){width="0 |                       |                       |
| .42777668416447945in" |                       |                       |
| > height="0.          |                       |                       |
| 42777668416447945in"} |                       |                       |
+=======================+=======================+=======================+
+-----------------------+-----------------------+-----------------------+

**Activities**

> •Collaborated on the aesthetic enhancement of TCS offices by painting
> walls and galleries.
>
> •Participated in CSR initiatives aimed at mentoring and teaching
> underprivileged students. •Utilized artistic skills to create
> caricature designs.
>
> •Organized team events to promote team building and foster a positive
> work environment. •Coordinated a large-scale event, the \'IT Wiz
> Quiz,\' which attracted over 2000 student participants.

**On-Site Exposure**

> •Traveled to various cities in China for work-related purposes on
> multiple occasions.
